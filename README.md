# Machine Learning braindump

Всякое-разное про машинное обучение в не особо структурированном виде. Владелец репозитория никому ничего не гарантирует и никакой ответственности за последствия от использования информации отсюда не несет.

# Оглавление
[Обзоры статей](#articles-overview)
  [Introduction to Semi-Supervised Learning with Ladder Networks](#a1-1)
  [Intro to optimization in deep learning: Busting the myth about batch normalization](#a1-2)
  [Layer Normalization](#a1-3)
  [Deep learning with Elastic Averaging SGD](#a1-4)
  [The Marginal Value of Adaptive Gradient Methods in Machine Learning](#a1-5)
  [Self-Normalizing Neural Networks](#a1-6)
  [The Two Phases of Gradient Descent in Deep Learning](#a1-7)
  [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](#a1-8)
  [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](#a1-9)

# Обзоры статей <a name="articles-overview"></a>

## Introduction to Semi-Supervised Learning with Ladder Networks <a name="a1-1"></a>

## Intro to optimization in deep learning: Busting the myth about batch normalization <a name="a1-2"></a>

## Layer Normalization <a name="a1-3"></a>

## Deep learning with Elastic Averaging SGD <a name="a1-4"></a>

## The Marginal Value of Adaptive Gradient Methods in Machine Learning <a name="a1-5"></a>

## Self-Normalizing Neural Networks <a name="a1-6"></a>

## The Two Phases of Gradient Descent in Deep Learning <a name="a1-7"></a>

## On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima <a name="a1-8"></a>

## Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour <a name="a1-9"></a>
